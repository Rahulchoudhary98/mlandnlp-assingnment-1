{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dbb18d2-0594-41dd-a138-d483464d81c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "df = pd.read_csv('spam.csv', encoding='ISO-8859-1') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f3fdf47-2085-4184-9c58-c90ea93ebafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe=df.iloc[:,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cbfff8c-0848-40ee-bc48-bbb9f050a92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul\\AppData\\Local\\Temp\\ipykernel_12484\\3288895809.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe.rename(columns={'v1':'label','v2':'message'},inplace=True)\n"
     ]
    }
   ],
   "source": [
    "dataframe.rename(columns={'v1':'label','v2':'message'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddaaac99-70de-44b5-bf24-8b95fac7882c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul\\AppData\\Local\\Temp\\ipykernel_12484\\1692912076.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe['label'] = dataframe['label'].map({'ham': 0, 'spam': 1})\n"
     ]
    }
   ],
   "source": [
    "dataframe['label'] = dataframe['label'].map({'ham': 0, 'spam': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a280bd01-0e8f-4dd5-8247-468eda8a8b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer ,WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cee15d8-feb9-43f0-80ef-df660fb8ce0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [Go, until, jurong, point, ,, crazy, .., Avail...\n",
      "1                [Ok, lar, ..., Joking, wif, u, oni, ...]\n",
      "2       [Free, entry, in, 2, a, wkly, comp, to, win, F...\n",
      "3       [U, dun, say, so, early, hor, ..., U, c, alrea...\n",
      "4       [Nah, I, do, n't, think, he, goes, to, usf, ,,...\n",
      "                              ...                        \n",
      "5567    [This, is, the, 2nd, time, we, have, tried, 2,...\n",
      "5568     [Will, Ì_, b, going, to, esplanade, fr, home, ?]\n",
      "5569    [Pity, ,, *, was, in, mood, for, that, ., So, ...\n",
      "5570    [The, guy, did, some, bitching, but, I, acted,...\n",
      "5571                  [Rofl, ., Its, true, to, its, name]\n",
      "Name: tokens, Length: 5572, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul\\AppData\\Local\\Temp\\ipykernel_12484\\1657836277.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe['tokens'] = dataframe['message'].apply(word_tokenize)\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')       # Primary tokenizer\n",
    "nltk.download('punkt_tab')  \n",
    "dataframe['tokens'] = dataframe['message'].apply(word_tokenize)\n",
    "print(dataframe['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "803c7945-36ac-4ed6-935a-9b0080c45316",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Users\\Rahul\\AppData\\Local\\Temp\\ipykernel_12484\\3843461603.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe['filtered_tokens'] = dataframe['tokens'].apply(\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words=set(stopwords.words('english'))\n",
    "dataframe['filtered_tokens'] = dataframe['tokens'].apply(\n",
    "    lambda tokens: [token.lower() for token in tokens if token.lower() not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44271572-b9e8-4645-b9a9-e76f73f73488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [go, jurong, point, ,, crazy, .., available, b...\n",
      "1                [ok, lar, ..., joking, wif, u, oni, ...]\n",
      "2       [free, entry, 2, wkly, comp, win, fa, cup, fin...\n",
      "3       [u, dun, say, early, hor, ..., u, c, already, ...\n",
      "4       [nah, n't, think, goes, usf, ,, lives, around,...\n",
      "                              ...                        \n",
      "5567    [2nd, time, tried, 2, contact, u., u, å£750, p...\n",
      "5568               [ì_, b, going, esplanade, fr, home, ?]\n",
      "5569           [pity, ,, *, mood, ., ..., suggestions, ?]\n",
      "5570    [guy, bitching, acted, like, 'd, interested, b...\n",
      "5571                                [rofl, ., true, name]\n",
      "Name: filtered_tokens, Length: 5572, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(dataframe['filtered_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e3975cc-588c-463f-b06c-cc7b3f5b942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=SnowballStemmer('english')\n",
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85cd6153-4190-46cf-80a1-06f63e5b4105",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['stemmed_tokens'] = dataframe['filtered_tokens'].apply(\n",
    "    lambda tokens: [stemmer.stem(token) for token in tokens]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ebb422d-b602-4f56-915e-fb7827fec99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['lemmatized_tokens']=dataframe['filtered_tokens'].apply(\n",
    "    lambda tokens: [stemmer.stem(token) for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6eae763d-9794-48f6-b4db-972a38595ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "w2vmodel = KeyedVectors.load_word2vec_format(\n",
    "    'C:/Users/Rahul/Downloads/GoogleNews-vectors-negative300.bin.gz',\n",
    "    binary=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ab23a0d-2664-41f6-a1ce-ac66a54f805f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_vector(tokens, model):\n",
    "    valid_vectors = [model[word] for word in tokens if word in model.key_to_index]\n",
    "    if valid_vectors:\n",
    "        return np.mean(valid_vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)  \n",
    "dataframe['vector'] = dataframe['stemmed_tokens'].apply(lambda tokens: get_average_vector(tokens, model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e950048-2ec4-4a2f-a900-a7a12b5e5a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: [[ 0.0880127  -0.0042038  -0.01313782 ... -0.00106001  0.06237984\n",
      "  -0.06787109]\n",
      " [ 0.02212341 -0.02541016 -0.01249634 ...  0.06154785 -0.06165283\n",
      "  -0.01927979]\n",
      " [-0.03916015  0.03823242  0.01474609 ... -0.04770508 -0.07602539\n",
      "   0.03994141]\n",
      " ...\n",
      " [-0.02246094  0.04408773  0.01045736 ...  0.02335612 -0.02492269\n",
      "   0.11832682]\n",
      " [-0.08770752  0.05360794  0.0501709  ... -0.14657593 -0.01367188\n",
      "   0.04309082]\n",
      " [-0.05167643  0.12972005 -0.14660645 ...  0.00791423  0.00520833\n",
      "   0.03597005]]\n",
      "X_test: [[ 0.0814233  -0.04077791  0.02900656 ... -0.07412559  0.02616481\n",
      "   0.07497366]\n",
      " [ 0.00352097 -0.05347824  0.0329504  ... -0.05103207  0.00681019\n",
      "  -0.00305462]\n",
      " [ 0.00771077 -0.03104655  0.0211792  ... -0.0602417   0.09738668\n",
      "  -0.00940959]\n",
      " ...\n",
      " [ 0.10205078 -0.05161133  0.04990234 ... -0.16689453  0.05258789\n",
      "  -0.05972214]\n",
      " [ 0.10758463 -0.06258138 -0.03580729 ... -0.05525716  0.07877604\n",
      "  -0.028361  ]\n",
      " [ 0.00712891  0.08352051 -0.11447754 ... -0.10012207 -0.0411499\n",
      "   0.07958984]]\n",
      "y_train: [0 1 0 ... 0 0 0]\n",
      "y_test: [0 0 1 ... 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.vstack(dataframe['vector'].values)  \n",
    "y = dataframe['label'].values              \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print('X_train:',X_train)\n",
    "print('X_test:',X_test)\n",
    "print('y_train:',y_train)\n",
    "print('y_test:',y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f71c4a97-cf89-4454-8af5-e39f421b5b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9345\n"
     ]
    }
   ],
   "source": [
    "clff= LogisticRegression(max_iter=1000)  \n",
    "clff.fit(X_train, y_train)\n",
    "y_pred = clff.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6267afa9-a83d-43ba-8409-dfd455614d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_message_class(model,w2vmodel, message):\n",
    "    import numpy as np\n",
    "    import re\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import SnowballStemmer\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    tokens = word_tokenize(message)\n",
    "    filtered = [stemmer.stem(w) for w in tokens if w not in stop_words]\n",
    "    valid_vectors = [w2vmodel[word] for word in filtered if word in w2vmodel.key_to_index]\n",
    "    if valid_vectors:\n",
    "        vector = np.mean(valid_vectors, axis=0).reshape(1, -1)\n",
    "    else:\n",
    "        vector = np.zeros(w2vmodel.vector_size).reshape(1, -1)\n",
    "\n",
    "    # Predict using the trained classifier\n",
    "    prediction = clff.predict(vector)[0]\n",
    "    return prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db0a227a-61b3-4dd6-b62b-a17ffd456bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "message = \"Congratulations You've won a free ticket. Text WIN to 12345.\"\n",
    "print(predict_message_class(model=dataframe, w2vmodel=model, message=message))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ce2e80-051c-4399-95f6-85ea9f37570f",
   "metadata": {},
   "source": [
    "# question 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50cee493-ddef-4eef-9996-49cb8efbfa82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\n",
      "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting textsearch>=0.0.21 (from contractions)\n",
      "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
      "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
      "  Downloading pyahocorasick-2.2.0-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
      "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
      "Downloading pyahocorasick-2.2.0-cp312-cp312-win_amd64.whl (35 kB)\n",
      "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.2.0 textsearch-0.0.24\n",
      "Collecting emoji\n",
      "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
      "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
      "   ---------------------------------------- 0.0/590.6 kB ? eta -:--:--\n",
      "   ----------------- ---------------------- 262.1/590.6 kB ? eta -:--:--\n",
      "   ----------------- ---------------------- 262.1/590.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 590.6/590.6 kB 1.0 MB/s eta 0:00:00\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.14.1\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions\n",
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bce4abf8-e37b-4f46-bc19-0d116c985856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "C:\\Users\\Rahul\\AppData\\Local\\Temp\\ipykernel_17856\\3214352847.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1['clean_text'] = df1['text'].apply(preprocess_tweet)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import contractions\n",
    "import emoji\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "df = pd.read_csv('Tweets.csv.zip')  \n",
    "df1 = df[['text', 'airline_sentiment']]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_tweet(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = contractions.fix(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    tokens = [token for token in tokens if not emoji.is_emoji(token)]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df1['clean_text'] = df1['text'].apply(preprocess_tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e3941d7-acb6-4b77-8b84-dcab3351b47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "w2vmodel = KeyedVectors.load_word2vec_format(\n",
    "    'C:/Users/Rahul/Downloads/GoogleNews-vectors-negative300.bin.gz',\n",
    "    binary=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "681c439f-6798-4ac9-90fa-fd0dc026ffb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def tweet_to_vec(tweet, model, dim=300):\n",
    "    words = tweet.split()\n",
    "    valid_words = [w for w in words if w in model]\n",
    "    if not valid_words:\n",
    "        return np.zeros(dim)\n",
    "    return np.mean([model[w] for w in valid_words], axis=0)\n",
    "\n",
    "X = np.vstack(df1['clean_text'].apply(lambda x: tweet_to_vec(x, w2vmodel)).values)\n",
    "y = df1['airline_sentiment'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a952ab55-231c-417a-81f5-4c6b9497e795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bcbc242d-fe4f-48e3-ba33-eb499cae834d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\New folder\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7835\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs')\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "671db692-2496-42fb-a778-48dccade864e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "def predict_tweet_sentiment(model, w2v_model, tweet):\n",
    "    clean = preprocess_tweet(tweet)\n",
    "    vec = tweet_to_vec(clean, w2v_model)\n",
    "    pred = model.predict([vec])[0]\n",
    "    return pred\n",
    "\n",
    "# Example \n",
    "tweet = \"I love flying with Delta, always a great experience!\"\n",
    "print(predict_tweet_sentiment(clf, w2vmodel, tweet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff4e645-2c46-4f85-a31b-538e1b66f4b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
